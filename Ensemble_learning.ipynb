{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPU6BZk9lCHXeCc8jvZ7jB0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceyxasm/ml/blob/main/Ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Learning**\n",
        "\n",
        "Suppose you build an ensemble containing 1,000 classifiers that are individ‐\n",
        "ually correct only 51% of the time (barely better than random guessing). If you pre‐\n",
        "dict the majority voted class, you can hope for up to 75% accuracy! However, this is\n",
        "only true if all classifiers are perfectly independent, making uncorrelated errors\n",
        "\n"
      ],
      "metadata": {
        "id": "rAYPJHQB8c2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_moons( 500, noise=0.035, random_state=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier(max_depth=4)\n",
        "svm_clf = SVC(C=0.05)\n",
        "voting_clf = VotingClassifier(\n",
        "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "voting='hard')\n",
        "voting_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnX15bm_8r8Z",
        "outputId": "3f9e6c08-669e-4005-e844-39413c820220"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
              "                             ('rf', RandomForestClassifier(max_depth=4)),\n",
              "                             ('svc', SVC(C=0.05))])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_fPjBUt9Cnf",
        "outputId": "2e2aa0a6-7c6e-4050-c1a1-1b776112418a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.896969696969697\n",
            "RandomForestClassifier 0.9878787878787879\n",
            "SVC 0.9636363636363636\n",
            "VotingClassifier 0.9636363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## implementing soft voting\n",
        "\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier(max_depth=4)\n",
        "svm_clf = SVC(C=0.05, probability=True) #<<<<<<<<<<<<<<<<<<<<<<<\n",
        "voting_clf = VotingClassifier(\n",
        "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "voting='soft')  #<<<<<<<<<<<<<<<<<<<<<<,  \n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUtTjzIx_v8V",
        "outputId": "e6698474-4c46-432b-a670-4c381dde5257"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.896969696969697\n",
            "RandomForestClassifier 0.9878787878787879\n",
            "SVC 0.9636363636363636\n",
            "VotingClassifier 0.9454545454545454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Clearly are ensemble is not performing as it should.\n",
        "reason being: we are training the models on same data. Errors will therefore be correlated.\n",
        "\n",
        " Using different classifier is one way of doing ensemble learning.\n",
        " \n",
        " .\n",
        " ----\n",
        "**Bagging and Pasting**\n",
        "\n",
        "use the same training algorithm for every\n",
        "predictor, but to train them on different random subsets of the training set. When\n",
        "sampling is performed with replacement, this method is called bagging . When sampling is performed without replacement, it is called\n",
        "pasting. "
      ],
      "metadata": {
        "id": "DDQXNijgDKRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier  #BaggingRegressor for regression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "  DecisionTreeClassifier(), n_estimators=500,\n",
        "  max_samples=100, bootstrap=True, n_jobs=-1) #for pasting, bootstrap=False\n",
        "'''here 500 Dtrees are trained on 100 randomly sampled instances'''\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "print('Accuracy:  ', accuracy_score(y_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5-qgcraCtkQ",
        "outputId": "67a54771-7084-4a8a-ae27-7a1dce3d6244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:   0.9878787878787879\n"
          ]
        }
      ]
    }
  ]
}